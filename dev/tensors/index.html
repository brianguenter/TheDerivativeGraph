<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>Tensor Derivatives · TheDerivativeGraph</title><script data-outdated-warner src="../assets/warner.js"></script><link href="https://cdnjs.cloudflare.com/ajax/libs/lato-font/3.0.0/css/lato-font.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/juliamono/0.045/juliamono.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.4/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.4/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.4/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.24/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL=".."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="../assets/documenter.js"></script><script src="../siteinfo.js"></script><script src="../../versions.js"></script><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/documenter-dark.css" data-theme-name="documenter-dark" data-theme-primary-dark/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="../assets/themeswap.js"></script></head><body><div id="documenter"><nav class="docs-sidebar"><div class="docs-package-name"><span class="docs-autofit"><a href="../">TheDerivativeGraph</a></span></div><form class="docs-search" action="../search/"><input class="docs-search-query" id="documenter-search-query" name="q" type="text" placeholder="Search docs"/></form><ul class="docs-menu"><li><a class="tocitem" href="../">The Derivative Graph and its use</a></li><li><a class="tocitem" href="../thederivativegraph/">The Derivative Graph</a></li><li class="is-active"><a class="tocitem" href>Tensor Derivatives</a></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><nav class="breadcrumb"><ul class="is-hidden-mobile"><li class="is-active"><a href>Tensor Derivatives</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href>Tensor Derivatives</a></li></ul></nav><div class="docs-right"><a class="docs-edit-link" href="https://github.com/brianguenter/TheDerivativeGraph/blob/main/docs/src/tensors.md" title="Edit on GitHub"><span class="docs-icon fab"></span><span class="docs-label is-hidden-touch">Edit on GitHub</span></a><a class="docs-settings-button fas fa-cog" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-sidebar-button fa fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a></div></header><article class="content" id="documenter-page"><p>To compute tensor derivatives efficiently we will use a symbolic index representation. This has several benefits:</p><ul><li>Unlike other tensor differentiation systems which may apply many complex tensor identities to simplify an expression we will use a single simple operation: tensor contraction.</li><li>In the symbolic index form all operations are scalar which gives the AD algorithm designer great flexibility to rearrange the order of operations. An important use of this feature is operator fusion. Another is tiling.</li><li>Tensor differentiation tends to create tensors that are sparse. In many cases this sparsity is explicitly represented by constraint equations on the symbolic indices. This dramatically reduces computation and memory usage.</li></ul><p>Computing tensor derivatives is conceptually simple, at least for tensor operations that can be represented as sequences of tensor contraction. This may seem restrictive but tensor contraction covers a lot of territory. </p><p>A tensor contraction is a summation along one or more indices. Using Einstein notation a contraction occurs whenever two terms have matching indices. For example, matrix-vector multiplication can be written as a tensor contraction</p><p class="math-container">\[\begin{aligned}
Ab &amp;= \sum\limits_{j} A_{ij}b{j} \\
&amp;= A_{ij}b_j
\end{aligned}\]</p><p>and matrix-matrix multiplication can be written as the tensor contraction <span>$A_{ij}B_{jk}$</span>. </p><p>Convolution is also a tensor contraction (we&#39;ll use the ⨀ symbol for convolution to distinguish it from *):</p><p class="math-container">\[\begin{aligned}
(s \odot f)_{i} &amp;= \sum\limits_{l} s_{i-l}f{l} \\
&amp;= s_{i-l}*f_l
\end{aligned}\]</p><p>Tensor differentiation proceeds in simple steps:</p><ul><li>Write out the indices on the tensor terms. Indices shared between two or more terms indicate a tensor contraction.</li><li>Transform the tensor expression into a function graph.</li><li>Transform the function graph into a derivative graph. Change tensor contractions into summation operation nodes.</li><li>Find the variable being differentiated with respect to and compute index substitutions. </li><li>Propagate the index substitutions up the graph.</li></ul><p>Let&#39;s do several examples first, before going into the details of why this works. The first example is: compute <span>$\frac{\partial f_i}{\partial A_{mn}}$</span> for <span>$f_i = A_{ij}b_j$</span>. </p><p>First create the function graph corresponding to the expression <span>$f_i = A_{ij}b_j$</span></p><p><img src="../illustrations/Ab/Ab_illustration.svg" alt="Ab"/></p><p>Now transform to a derivative graph, where <span>$Df_{imn}$</span> refers to the derivative <span>$\frac{\partial f_i}{\partial A_{mn}}$</span> indexed at <span>$imn$</span>,</p><p><img src="../illustrations/Ab/Ab_illustrationD.svg" alt="Ab_deriv"/> <img src="../illustrations/Ab/Ab_partial_pathD.svg" alt="Ab_deriv"/></p><p>In the graph on the right we&#39;ve grayed out edges and nodes which don&#39;t contribute to <span>$\frac{\partial f_i}{\partial A_{mn}}$</span>.</p><p>Now locate the graph variable corresponding to the variable being differentiated with respect to. This is the node labeled <span>$A_{ij}$</span>. </p><p>Create a substitution rule to replace the <span>$i,j$</span> indices in <span>$A_{ij}$</span> with <span>$m,n$</span>: <span>$sub((i=m,j=n))$</span>. Apply the substitution to the graph nodes and edges on the product path from <span>$f_i$</span> to <span>$A_{ij}$</span>, beginning with <span>$A_{ij}$</span> and working upward:</p><p><img src="../illustrations/Ab/Ab_partial_Aij_step1D.svg" alt="Ab_deriv1"/> <img src="../illustrations/Ab/Ab_partial_Aij_step2D.svg" alt="Ab_deriv2"/> <img src="../illustrations/Ab/Ab_partial_Aij_step3D.svg" alt="Ab_deriv3"/> <img src="../illustrations/Ab/Ab_partial_Aij_step4D.svg" alt="Ab_deriv4"/></p><p>Notice that the substition <span>$sub((i=m,j=n),\sum\limits{j})$</span> collapses to a no-op. This is because the summation is zero except when <span>$j=n$</span>; there is only one term in the summation. </p><p><img src="../illustrations/Ab/Ab_partial_Aij_step5D.svg" alt="Ab_deriv5"/></p><p>In the final substitution change you will notice that the first index of <span>$Df$</span> is an equality constraint caused by the substition rule <span>$sub((i=m,j=n),...)$</span>. All terms <span>$Df_{i,m,n}$</span> with <span>$i \ne m$</span> are identically 0.</p><p>Now multiply all the terms on the product path from node <span>$Df_{i=m,mn}$</span> to node <span>$A_{mn}$</span>. This product is <span>$Df_{i=m,mmn} = 1*1*b_n = b_n$</span>. </p><p>Although the derivative has three indices <span>$i,m,n$</span> the only non-zero elements of this tensor are the elements of <span>$b_n$</span>. Storing the tensor derivative takes space proportional to the size of <span>$b_n$</span>. This reduction in storage happens frequently when taking tensor derivatives.</p><p>Let&#39;s test the symbolic index result we&#39;ve just computed by writing a FastDifferentiation function to compute the derivative symbolically:</p><pre><code class="language-julia hljs">function Ab()
    A = make_variables(:A, 2, 2)
    b = make_variables(:b, 2)

    jac = jacobian(FD.Node.(A * b), vec(A))
    reshape(jac, 2, 2, 2)
end
export Ab</code></pre><p>and here&#39;s the evaluation:</p><pre><code class="language-julia hljs">julia&gt; Ab()
2×2×2 Array{FastDifferentiation.Node, 3}:
[:, :, 1] =
  b1  0.0
 0.0   b1

[:, :, 2] =
  b2  0.0
 0.0   b2</code></pre><p>As expected the derivative is non-zero only when the first two indices are equal.</p><p>In the first example the summation operator <span>$\sum\limits_{j}$</span> disappeared because the summation index and the substitution index were the same. The summation is only non-zero when the summation index equals the substitution so the summation collapse to a single term. </p><p>The next example shows a case where this is not true: given <span>$f_k = B_{ki}A_{ij}x_j$</span> compute <span>$\frac{\partial f_k}{\partial x_n}$</span>.</p><p>First create the function graph corresponding to <span>$f_k = B_{ki}A_{ij}x_j$</span> <img src="../illustrations/BAx/BAx_illustration.svg" alt="BAx"/></p><p>Now transform to a derivative graph and gray out edges and nodes which do not contribute to the derivative: <img src="../illustrations/BAx/BAx_partial_xjD.svg" alt="BAx partial"/></p><p>Create the substitution rule by locating  by locating the variable node <span>$x_j$</span>.</p><p><img src="../illustrations/BAx/BAx_partial_xj_step1D.svg" alt="BAx partial"/></p><p>Apply the substitution rule up the graph</p><p><img src="../illustrations/BAx/BAx_partial_xj_step2D.svg" alt="BAx partial"/> <img src="../illustrations/BAx/BAx_partial_xj_step3D.svg" alt="BAx partial"/></p><p>No terms higher in the graph than the <span>$\sum\limits_{j}$</span> node use the index <span>$j$</span> so none of them are affected by the substitution rule <span>$sub((j=n),...)$</span>.  </p><p><img src="../illustrations/BAx/BAx_partial_xj_step4D.svg" alt="BAx partial"/></p><p>The final result is</p><p class="math-container">\[\frac{\partial f_k}{\partial x_n} = Df_{kn}= \sum\limits_{i} B_{ki}A_{in}\]</p><h3 id="Why-does-it-work?"><a class="docs-heading-anchor" href="#Why-does-it-work?">Why does it work?</a><a id="Why-does-it-work?-1"></a><a class="docs-heading-anchor-permalink" href="#Why-does-it-work?" title="Permalink"></a></h3><p class="math-container">\[\frac{\partial b_j}{\partial b_k} = \begin{cases}
0  &amp; j \ne k, \\
1 &amp; j=k
\end{cases}\]</p></article><nav class="docs-footer"><a class="docs-footer-prevpage" href="../thederivativegraph/">« The Derivative Graph</a><div class="flexbox-break"></div><p class="footer-message">Powered by <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> and the <a href="https://julialang.org/">Julia Programming Language</a>.</p></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> version 0.27.25 on <span class="colophon-date" title="Saturday 26 August 2023 00:09">Saturday 26 August 2023</span>. Using Julia version 1.9.3.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></html>
